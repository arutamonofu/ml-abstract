Метод опорных векторов (Support Vector Machine, SVM) — линейный классификатор (и регрессор), целью которого является построение разделяющей гиперплоскости в пространстве признаков таким образом, чтобы зазор (margin) между этой гиперплоскостью и ближайшими к ней точками разных классов (опорными векторами) был максимальным. Для нелинейных задач SVM использует «ядровой трюк» (kernel trick) для отображения данных в пространство более высокой размерности, где их можно разделить линейно.

| Характеристика               | Значение               |
| :--------------------------- | :--------------------- |
| Задача                       | Бинарная классификация |
| Тип обучения                 | Обучение с учителем    |
| Множество целевой переменной | $\{-1, 1\}$            |

# Обучение

Прямая задача — поиск таких $\mathbf{w}$ и $b$, которые максимизируют зазор $\frac{2}{||w||}$ и минимизируют ошибку: $$\min_{w,b,\{\xi_i\}} \left[ \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{N} \xi_i \right]$$
при условиях:

- $y_i(\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \forall i=1, \dots, N$
- $\xi_i \geq 0, \quad \forall i=1, \dots, N$

Алгоритм:

1. Если зависимость нелинейная (объекты нельзя разделить гиперплоскостью), выбрать и применить одну из ядерных функций похожести к каждой паре объектов $K(x_i, x_j)$, неявно преобразующую данные в пространство высшей размерности, в котором разделение гиперплоскостью становится возможным.
2. Преобразовать прямую задачу в двойственную задачу поиска вектора множителей Лагранжа $\mathbf{\alpha}$, использующую $K(x_i, x_j)$: $$\max_{\boldsymbol{\alpha}} \left[ \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j) \right]$$
   при условиях:
	   - $\sum_{i=1}^{N} \alpha_i y_i = 0$
	   - $0 \leq \alpha_i \leq C, \quad \forall i=1, \dots, N$
3. Посредством метода Sequential Minimal Optimization решить двойственную задачу.

## Гиперпараметры

Подход к обучению:

* Тип ядра (kernel): определяет вид преобразования признаков (линейное, радиально-базисная функция (RBF/гауссово), полиномиальное, сигмоидное).
	* Для RBF-ядра: параметр гамма ($\gamma$), определяющий радиус влияния одного объекта (насколько далеко распространяется влияние одного тренировочного примера).
	* Для полиномиального ядра: степень полинома (degree).
	* Для полиномиального и сигмоидного ядер: свободный член (coef0).
* Эвристика сжатия (shrinking): использовать ли эвристику для ускорения обучения.

Регуляризация:

* Параметр штрафа за ошибку ($C$): регулирует баланс между максимизацией ширины зазора и минимизацией ошибок классификации на тренировочных данных. Большие значения $C$ приводят к меньшему количеству ошибок, но могут увеличить переобучение.

Условия остановки:

* Допуск (tolerance): критерий остановки для оптимизатора, определяющий минимальное изменение функции потерь для продолжения обучения.
* Максимальное количество итераций (max_iter): максимальное число итераций для оптимизационного алгоритма.

# Предсказание

$$f(\mathbf{x}_{\text{new}}) = \text{sign}\left( \sum_{i : \alpha_i > 0} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}_{\text{new}}) + b \right)$$


# Оценка качества

- [[Метрики качества классификаторов]]