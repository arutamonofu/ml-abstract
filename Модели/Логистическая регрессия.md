![[logistic_regression.png]]

Логистическая регрессия (logistic regression) — линейный вероятностный классификатор для задачи бинарной классификации, который моделирует вероятность принадлежности объекта к классу путём преобразования линейной комбинации признаков с помощью логистической функции (сигмоиды), подбирая коэффициенты для максимизации правдоподобия наблюдаемых данных.

# Свойства

| Характеристика               | Значение            |
| ---------------------------- | ------------------- |
| Задача                       | Классификация       |
| Множество целевой переменной | $[0, 1]$            |
| Тип машинного обучения       | Обучение с учителем |

# Обучение

Два подхода к обучению:

- один против всех,
- мультиномиальный (softmax).

---

**Один против всех** 

Пусть есть $N$ объектов и $K$ классов. Для каждого класса $k$ обучается отдельный бинарный классификатор, отличающий $k$ от остальных классов.

Алгоритм обучения бинарного классификатора:

1. Инициализировать случайно веса $\hat{\mathbf{w}}$ и смещение $\hat{b}$.
2. Для каждого объекта $i$ вычислить сигмоидную функцию от логита — вероятность $k$: 
   $$\hat{y} = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}$$
3. Вычислить функцию потерь — среднее бинарной кросс-энтропии:
   $$\text{MBCE} = \frac{1}{N}\sum_i^N{[-y_i\ln(\hat{y_i}) - (1 - y_i)\ln(1 - \hat{y_i})]}$$
4. Оптимизировать веса и смещение с помощью градиентного спуска в соответствии со скоростью обучения:

   $$\hat{b}' \leftarrow \hat{b} - \eta \frac{\partial \text{MBCE}}{\partial \hat{b}}$$$$\hat{w}'_j = \hat{w}_j - \eta \frac{\partial \text{MBCE}}{\partial \hat{w}_i}$$
5. Повторять шаги 2–4, пока не выполнится одно из условий остановки.

---

**Мультиномиальный (softmax)**

Пусть есть $N$ объектов и $K$ классов.

1. Для каждого класса $k$ случайно инициализировать вектор весов $\mathbf{\hat{w}}_k$ и смещение $\hat{b}_k$. Из векторов весов составить матрицу $\hat{\mathbf{W}}$, из скаляров смещений составить вектор $\hat{\mathbf{b}}$.
2. Для каждого объекта $i$ для каждого класса $k$ вычислить $\text{softmax}$ от линейной оценки:
   $$ z_k = \mathbf{w}_k^T \mathbf{x}_i + b_k$$
   $$\text{softmax}(z_k) = \frac{e^{z_k}}{\sum_{k=1}^K e^{z_k}} = \hat{p}_k$$
3. Вычислить функцию потерь — среднее многоклассовой кросс-энтропии:
   $$\text{LogLoss} = -\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^K y_{i,k} \log (\hat{p}_{i, k}),$$
   где $y_{i, k} \in \{0, 1\}$ — индикатор истинной принадлежности объекта $i$ к классу $k$.
4. Оптимизировать $\mathbf{W}$ и $\mathbf{b}$ с помощью градиентного спуска (по умолчанию метод L-BFGS).
5. Повторять шаги 2–4, пока не выполнится одно из условий остановки.

## Гиперпараметры

Подход к обучению:

* Алгоритм оптимизации;
* Веса для классов при несбалансированных данных;
* Добавлять ли смещение;
* Продолжать ли обучение с предыдущего состояния модели;
* Стратегия для многоклассовой классификации: OvR или softmax.

Регуляризация:

*   Тип регуляризации;
*   Обратный коэффициент регуляризации ($1/\lambda$);
*   Параметр смешивания для регуляризации elastic net.

Условия остановки:

*   Максимальное количество итераций обучения;
*   Минимальное изменение функции потерь для продолжения обучения.

## Функции потерь

Среднее логарифмических потерь ($MBCE$):

$$\text{MBCE} = \frac{1}{N}\sum_i^N{[-y_i\ln(\hat{y_i}) - (1 - y_i)\ln(1 - \hat{y_i})]}$$

> [!Info]
> Бинарная кросс-энтропия ($L$, или $BCE$):
> 
>$$-y\ln(\hat{y}) - (1 - y)\ln(1 - \hat{y})$$
>
> Эта формула работает так. Если метка равна 0, логарифмическая потеря $-\ln(1 - \hat{y})$. Если метка равна 1, логарифмическая потеря $-\ln(\hat{y})$. Если метка равна 0, то первое слагаемое равно 0, а если метка равна 1, то второе слагаемое равно 0.

# Оценка качества

- [[Метрики качества классификаторов]]
- [[ROC-кривая]]