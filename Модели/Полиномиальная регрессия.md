Полиномиальная регрессия (polynomial regression) — это вид линейной регрессии, который моделирует нелинейную зависимость между независимыми переменными (признаками) и зависимой переменной. Она достигается путём преобразования исходных признаков в полиномиальные признаки заданной степени, после чего к этим новым признакам применяется стандартная линейная регрессия для подбора коэффициентов (весов) и смещения таким образом, чтобы минимизировать сумму квадратов разностей между фактическими и предсказанными значениями.

| Характеристика               | Значение            |
| ---------------------------- | ------------------- |
| Задача                       | Регрессия           |
| Тип обучения                 | Обучение с учителем |
| Множество целевой переменной | $\mathbb{R}$        |

# Обучение

Алгоритм с использованием метода наименьших квадратов:

1. Преобразовать вектор исходных признаков в матрицу полиномиальных признаков желаемой степени $\mathbf{X}$: $$\mathbf{X} = \begin{pmatrix} 1 & x_1 & x_1^2 & \dots & x_1^k \\ 1 & x_2 & x_2^2 & \dots & x_2^k \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_n & x_n^2 & \dots & x_n^k \end{pmatrix}$$
2.  Инициализировать случайно веса $\hat{\mathbf{w}}$ (для преобразованных признаков) и смещение $\hat{b}$.
3.  Для каждого объекта $i$ вычислить предсказанное значение $\hat{y}_i$: $$\hat{y}_i = \hat{\mathbf{w}}_i^T \mathbf{x}_i + \hat{b}_i$$
4.  Вычислить функцию потерь — среднеквадратичную ошибку (MSE): $$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$
5. Оптимизировать веса и смещение (для преобразованных признаков) с помощью градиентного спуска в соответствии со скоростью обучения: $$\hat{w}_j \leftarrow \hat{w}_j - \eta \frac{\partial \text{MSE}}{\partial \hat{w}_j}$$ $$\hat{b} \leftarrow \hat{b} - \eta \frac{\partial \text{MSE}}{\partial \hat{b}}$$
6.  Повторять шаги 3–5, пока не выполнится одно из условий остановки.

## Гиперпараметры

Подход к обучению:

* Степень полинома;
* Алгоритм оптимизации (например, метод наименьших квадратов, градиентный спуск, стохастический градиентный спуск, пакетный градиентный спуск);
* Добавлять ли смещение;
* Продолжать ли обучение с предыдущего состояния модели.

Регуляризация:

* Тип регуляризации (L1, L2, Elastic Net);
* Коэффициент регуляризации.

Условия остановки:

* Максимальное количество итераций обучения;
* Минимальное изменение функции потерь для продолжения обучения.

## Функции потерь

| Название                                         | Описание                                                                                                                                                                   | Обозначение  | Формула                                                                                               |
| ------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | ----------------------------------------------------------------------------------------------------- |
| Средняя квадратичная ошибка (Mean Squared Error) | Вычисляет среднее значение квадратов разностей между фактическими и предсказанными значениями. Очень чувствительна к большим ошибкам (выбросам).                           | $\text{MSE}$ | $\frac{\sum_{i=1}^{N} \left( y_i - \hat{y}_i \right)^2}{N}$                                           |
| Ошибка регрессии LASSO                           | Состоит из MSE и L1-штрафа (сумма модулей весов). Штрафует модель за сложность и способна обнулять коэффициенты неважных признаков, выполняя их отбор.                     | $\text{L1}$  | $\frac{\sum_{i=1}^{N} \left( y_i - \hat{y}_i \right)^2}{N} + \lambda \sum_{j=1}^{m}\lvert w_j \rvert$ |
| Ошибка гребневой регрессии                       | Состоит из MSE и L2-штрафа (сумма квадратов весов). Штрафует модель за большие коэффициенты, делая ее более устойчивой к переобучению, но обычно не обнуляет их полностью. | $\text{L2}$  | $\frac{\sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2}{N} + \lambda \sum_{j=1}^{m}w_j^2$             |

# Предсказание

$$\hat{y} = \hat{\mathbf{w}}^T \mathbf{x} + \hat{b}$$

# Оценка качества

- [[Метрики качества регрессоров]]