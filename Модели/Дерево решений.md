![[decision_tree.png]]

Дерево решений (decision tree) — непараметрический алгоритм для задач классификации и регрессии, который строит иерархическую структуру из логических правил, последовательно разбивая данные на подмножества для достижения максимальной однородности по целевой переменной в конечных узлах.

| Характеристика               | Значение                                                     |
| ---------------------------- | ------------------------------------------------------------ |
| Задача                       | Регрессия, многоклассовая классификация                      |
| Тип обучения                 | Обучение с учителем                                          |
| Множество целевой переменной | Регрессия: $\mathbb{R}$ <br>Классификация: $\{0, \dots, K\}$ |

# Обучение

Алгоритм:

1. Инициализировать корневой узел — исходный набор данных.
2. Для каждого количественного признака отсортировать уникальные значения, для каждых двух соседних вычислить средние (по ним будут разделения). Для каждого мультиномиального категориального признака создать фиктивные бинарные.
3. Для каждого варианта разделения рассчитать индекс Джини или прирост информации.
4. Для каждого узла выполнить разделение с наименьшим индексом Джини или наибольшим приростом информации.
5. Повторять шаги 2–4 до выполнения условий остановки, или пока модель не классифицирует все объекты однозначно.

## Гиперпараметры

### Обучение

Критерий оценки разделения:

| Критерий              | Описание                                                                                                              | Формула                                                                                                                     |
| --------------------- | --------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| Индекс примесей Джини | Вероятность взять случайным образом из набора 2 объекта разных классов.                                               | Для одного узла:<br>$G(Y) = 1 - \sum{p_i^2}$<br><br>Для пары узлов:<br>$G(Y, X) = \frac{n_1}{N}G_1 + \frac{n_2}{N}G_2$      |
| Энтропия              | Как бы соотносится с вероятностью получить заданную последовательность всех объектов, доставая их случайно из набора. | Для одного узла: $E(Y) = - \sum{p_i \log_2(p_i)}$<br><br>Для пары узлов:<br>$E(Y, X) = \frac{n_1}{N}E_1 + \frac{n_2}{N}E_2$ |

Другие гиперпараметры:

- Выбор лучшего разделения по всем признакам или случайному подмножеству.
- Веса классов при несбалансированной выборке.

### Условия остановки и борьба с переобучением:

- Максимальная глубина.
- Максимальное количество листьев.
- Минимальное число объектов в узле для разделения узла.
- Минимальное число объектов в листьях для разделения узла.
- Максимальное число признаков, учитываемых при каждом разбиении.
- Минимальное изменение оценки разделения при разделении.
- Минимальная доля объектов от всех в одном листе.
- Коэффициент регуляризации.

# Предсказание

## Классификация

В листе вероятность $p_k$ класса $k$ равна доле объектов класса $k$. Предсказывается класс с наибольшим $p_k$.

## Регрессия

В листе предсказывается среднее (или медианное) значение целевой переменной всех объектов обучающей выборки, попавших в этот лист.

# Оценка качества

## Классификация

- [[Метрики качества классификаторов]]
- [[ROC-кривая]]

## Регрессия

- [[Метрики качества регрессоров]]