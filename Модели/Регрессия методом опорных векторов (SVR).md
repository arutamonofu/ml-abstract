SVR (Support Vector Regression) — это адаптация метода опорных векторов (SVM) для задач регрессии. Целью SVR является поиск функции, которая наилучшим образом аппроксимирует зависимость в данных, оставаясь при этом максимально «плоской» (минимизируя сложность модели). Для этого SVR стремится найти функцию, которая отклоняется от обучающих данных не более чем на заданную величину $\xi$, при этом минимизируя штраф за точки, выходящие за пределы этой трубки. Для нелинейных задач SVM использует «ядровой трюк» (kernel trick) для отображения данных в пространство более высокой размерности, где их можно разделить линейно.

| Характеристика               | Значение            |
| :--------------------------- | :------------------ |
| Задача                       | Регрессия           |
| Тип обучения                 | Обучение с учителем |
| Множество целевой переменной | $\mathbb{R}$        |

# Обучение

Прямая задача — поиск таких $\mathbf{w}$ и $b$, которые минимизируют сложность модели $\frac{1}{2||w||}$ и ошибку:
$$
\min_{w,b,\{\xi_i, \xi_i^*\}} \left[ \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{N} (\xi_i + \xi_i^*) \right]
$$
при условиях:

- $y_i - (\mathbf{w}^T \mathbf{x}_i + b) \leq \varepsilon + \xi_i, \quad \forall i=1, \dots, N$
- $(\mathbf{w}^T \mathbf{x}_i + b) - y_i \leq \varepsilon + \xi_i^*, \quad \forall i=1, \dots, N$
- $\xi_i, \xi_i^* \geq 0, \quad \forall i=1, \dots, N$

Алгоритм:

1. Если зависимость нелинейная, выбрать и применить одну из ядерных функций похожести к каждой паре объектов $K(x_i, x_j)$, неявно преобразующую данные в пространство высшей размерности, в котором зависимость может быть описана линейной функцией.
2. Преобразовать прямую задачу в двойственную задачу поиска двух векторов множителей Лагранжа $\boldsymbol{\alpha}$ и $\boldsymbol{\alpha}^*$, использующую $K(x_i, x_j)$:
   $$
   \max_{\boldsymbol{\alpha}, \boldsymbol{\alpha}^*} \left[ \sum_{i=1}^{N} y_i(\alpha_i - \alpha_i^*) - \varepsilon \sum_{i=1}^{N}(\alpha_i + \alpha_i^*) - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) K(x_i, x_j) \right]
   $$
   при условиях:
	   - $\sum_{i=1}^{N} (\alpha_i - \alpha_i^*) = 0$
	   - $0 \leq \alpha_i, \alpha_i^* \leq C, \quad \forall i=1, \dots, N$
3. Посредством метода Sequential Minimal Optimization решить двойственную задачу.

## Гиперпараметры

Подход к обучению:

* Тип ядра (kernel): определяет вид преобразования признаков (линейное, радиально-базисная функция (RBF/гауссово), полиномиальное, сигмоидное).
	* Для RBF-ядра: параметр гамма ($\gamma$), определяющий радиус влияния одного объекта (насколько далеко распространяется влияние одного тренировочного примера).
	* Для полиномиального ядра: степень полинома.
	* Для полиномиального и сигмоидного ядер: свободный член.
* Эвристика сжатия (shrinking): использовать ли эвристику для ускорения обучения.

Регуляризация:

* Параметр штрафа за ошибку (C): регулирует баланс между минимизацией сложности модели и минимизацией ошибок на тренировочных данных.

Условия остановки:

* Допуск (tolerance): критерий остановки для оптимизатора, определяющий минимальное изменение функции потерь для продолжения обучения.
* Максимальное количество итераций: максимальное число итераций для оптимизационного алгоритма.

# Предсказание

$$f(\mathbf{x}_{\text{new}}) = \sum_{i=1}^{N} (\alpha_i - \alpha_i^*) K(\mathbf{x}_i, \mathbf{x}_{\text{new}}) + b$$

# Оценка качества

- [[Метрики качества регрессоров]]